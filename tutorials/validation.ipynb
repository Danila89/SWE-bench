{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec113f8e-56c1-44f7-9571-292c946bb6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/swe-bench/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob, json, os, sys\n",
    "sys.path.append('/root/SWE-bench')\n",
    "sys.path.append('/root/SWE-bench/swebench/metrics') # TODO: Replace with path to `SWE-bench/swebench/metrics` folder\n",
    "from conversion import convert_log_to_ground_truth\n",
    "from getters import get_logs_gold\n",
    "from monitor import monitor_validation, monitor_logs_same_diff\n",
    "sys.path = sys.path[:-1]\n",
    "\n",
    "sys.path.append('/root/SWE-bench/swebench/harness') # TODO: Replace with path to `SWE-bench/swebench/harness` folder\n",
    "from utils import has_attribute_or_import_error\n",
    "sys.path = sys.path[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e965b7-702e-49d4-bc15-2619dc185752",
   "metadata": {},
   "source": [
    "Declare repository; Fetch tasks, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c088f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob \n",
    "\n",
    "# logs_list = glob.glob(\"/mnt/llm/home/ibragim-bad/data/swe_large/log_dir/*.log\") + glob.glob(\"/mnt/llm/home/ibragim-bad/data/swe_large_new/log_dir/*.json\")\n",
    "# inst = [l.split('/')[-1].split('.')[0] for l in  logs_list]\n",
    "# with open(\"/root/SWE-bench/swe_like_111k.json\") as f:\n",
    "#     tasks = json.load(f)\n",
    "# cut_tasks = [t for t in tasks if t['instance_id'] not in set(inst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a328a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# ct_dict = defaultdict(list)\n",
    "\n",
    "# for t in cut_tasks:\n",
    "#     ct_dict[t['repo']].append(t)\n",
    "\n",
    "# mx_len = max([len(v) for v in ct_dict.values()])\n",
    "# new_ct = []\n",
    "# for i in range(mx_len):\n",
    "#     for k in ct_dict:\n",
    "#         if i < len(ct_dict[k]):\n",
    "#             t = ct_dict[k][i]\n",
    "#             new_ct.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89234aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8699ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 500\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(0, len(new_ct), N):\n",
    "#     with open(f\"/root/data/swe/tasks_{i//N}.json\", \"w\") as f:\n",
    "#         json.dump(new_ct[i:i+N], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194a1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = open(\"/root/data/progress.txt\").readlines()\n",
    "lst = set([l.strip() for l in lst])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "tasks = []\n",
    "tasks_dict = defaultdict(set)\n",
    "for l in lst:\n",
    "    with open(f\"/root/data/swe/{l}\") as f:\n",
    "        cur_t =  json.load(f)\n",
    "        tasks +=cur_t\n",
    "        for t in cur_t:\n",
    "            tasks_dict[l.split('/')[0]].add(t['instance_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96c8353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# l = {'swebench': {'resolved': {'count': 7, 'turns': {'avg': 9.571428571428571, 'median': 9.0, 'p90': 13.4}, 'opened_files': {'avg_precision': 0.8571428571428571, 'avg_recall': 1.0, 'avg_jaccard_similarity': 0.8571428571428571}, 'edit_loop_fraction': 0.14285714285714285, 'exit_cost_fraction': 0.0, 'patched_files': {'avg_precision': 0.9285714285714286, 'avg_recall': 1.0, 'avg_jaccard_similarity': 0.9285714285714286}}, 'failed': {'count': 88, 'turns': {'avg': 22.545454545454547, 'median': 16.5, 'p90': 45.3}, 'opened_files': {'avg_precision': 0.3901515151515151, 'avg_recall': 0.4431818181818182, 'avg_jaccard_similarity': 0.3901515151515151}, 'edit_loop_fraction': 0.25, 'exit_cost_fraction': 0.0, 'patched_files': {'avg_precision': 0.369047619047619, 'avg_recall': 0.45454545454545453, 'avg_jaccard_similarity': 0.369047619047619}}, 'skipped': {'count': 191, 'turns': {'avg': 39.02094240837696, 'median': 30.0, 'p90': 82.0}, 'opened_files': {'avg_precision': 0.22168638516806055, 'avg_recall': 0.31413612565445026, 'avg_jaccard_similarity': 0.22168638516806055}, 'edit_loop_fraction': 0.35602094240837695, 'exit_cost_fraction': 0.0}, 'ignored': {'count': 6}, 'total': {'count': 292}}}\n",
    "# print(json.dumps(l, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f1e9497-6e60-4027-8cee-790c8d0f1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'pydicom/pydicom' # TODO: Replace with repository name\n",
    "log_dir = '/root/data/log_dir_2' # TODO: Replace with path to folder of execution logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba9ae3b-726c-40ec-8ed2-4012fc4f70dc",
   "metadata": {},
   "source": [
    "Get map of version to setup commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa9d915-da85-4ca0-bba1-cd61ffeef3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks = json.load(open('path/to/pydicom-task-instances.json')) # TODO: Replace with path to versioned candidate task instances\n",
    "for t in tasks:\n",
    "    t[\"version\"] = \"0.0\"\n",
    "\n",
    "\n",
    "tasks = sorted(tasks, key=lambda x: x['created_at'], reverse=True)\n",
    "\n",
    "version_to_setup_commit = {}\n",
    "for t in tasks:\n",
    "    if 'version' in t and t['version'] not in version_to_setup_commit:\n",
    "        version_to_setup_commit[t['version']] = t['base_commit']\n",
    "assert(\n",
    "    sorted(list([x or \"\" for x in version_to_setup_commit.keys()])) ==\n",
    "    sorted(list(set([t['version'] or \"\" for t in tasks if 'version' in t])))\n",
    ")\n",
    "tasks = {t['instance_id']: t for t in tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994a4a9-fd3d-4554-90b7-24b10c499b8a",
   "metadata": {},
   "source": [
    "#### Monitor Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5fcd54c-1a45-41f9-8a9c-64e9ff9df31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Attempts: 22687\n",
      "Failed: 14862\n",
      "Usable: 7825\n",
      "Corrupt Test: 138\n",
      "Corrupt Diff: 80\n",
      "Test Script Timeout: 64\n",
      "Success E2E: 7543\n"
     ]
    }
   ],
   "source": [
    "failed_install, corrupt_test_patch, corrupt_patch, timeout, success = monitor_validation(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48cecde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13682"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([f for f in failed_install if '0.0' not in f]) + len(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c71b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "\n",
    "# fails = set([os.path.basename(t).split('.')[0] for t in failed_install if '0.0' not in t])\n",
    "\n",
    "# for t in sorted(tasks_dict):\n",
    "#     print(t, len(tasks_dict[t]), len(tasks_dict[t] & fails))\n",
    "\n",
    "# tasks_dict['tasks_0.json'] & fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ac198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecf576b3-22ed-4d8c-a40d-340ae8c56db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs same: 19909\n",
      "Logs diff: 3082\n"
     ]
    }
   ],
   "source": [
    "logs_same, logs_diff = monitor_logs_same_diff(\"/root/data/swe_large/log_dir\")\n",
    "print(f\"Logs same: {len(logs_same)}\")\n",
    "print(f\"Logs diff: {len(logs_diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49c099bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a74a5-c56e-4724-acf5-378b601a2d89",
   "metadata": {},
   "source": [
    "#### Get [FP]2[FP] Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb935466-43e8-45a7-8f46-15925d82312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_id_to_gt = {}\n",
    "for d in logs_diff:\n",
    "    status_gt = convert_log_to_ground_truth(d[0])\n",
    "    inst_id_to_gt[d[0]] = status_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa7863-f144-49da-ae92-6430841bd3ea",
   "metadata": {},
   "source": [
    "#### Create Task Instances `.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44a4dc43-d158-4b3e-b7cd-728954bce188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of task instances:  0\n"
     ]
    }
   ],
   "source": [
    "tasks_final = []\n",
    "get_id_from_log = lambda x: x.split('/')[-1].split('.')[0]\n",
    "for k, v in inst_id_to_gt.items():\n",
    "    if len(v['FAIL_TO_PASS']) == 0:\n",
    "        continue\n",
    "    if not get_id_from_log(k) in tasks:\n",
    "        continue\n",
    "    task = tasks[get_id_from_log(k)]\n",
    "    task['FAIL_TO_PASS'] = v['FAIL_TO_PASS']\n",
    "    task['PASS_TO_PASS'] = v['PASS_TO_PASS']\n",
    "    task['environment_setup_commit'] = version_to_setup_commit[\"0.0\"]\n",
    "\n",
    "    # Do not consider tasks where the log before the patch has an attribute/import error\n",
    "    log_path = os.path.join(log_dir, f'{task[\"instance_id\"]}.log')\n",
    "    log_before, log_after = get_logs_gold(log_path)\n",
    "    if has_attribute_or_import_error(log_before):\n",
    "        continue\n",
    "\n",
    "    tasks_final.append(task)\n",
    "print(f\"Final number of task instances: \", len(tasks_final))\n",
    "\n",
    "# SAVE_PATH = \"path/to/save/tasks/to.json\" # TODO: Replace this with a path to a .json file to save the task instances\n",
    "# with open(SAVE_PATH, 'w') as f:\n",
    "#     json.dump(tasks_final, fp=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e14fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
